{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375f1c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:05.482201Z",
     "start_time": "2022-07-25T08:33:59.198182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading tokenizers/punkt/spanish.pickle: Package\n",
      "[nltk_data]     'tokenizers/punkt/spanish.pickle' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# Text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SpanishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('tokenizers/punkt/spanish.pickle')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e5bb89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:08.189064Z",
     "start_time": "2022-07-25T08:34:08.159352Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenize_spanish = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_stemmer = SpanishStemmer()\n",
    "spanish_stopwords_th = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1181bc44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:21.870583Z",
     "start_time": "2022-07-25T08:34:21.864568Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_text(text, subject_name, \n",
    "               tokenize_spanish=tokenize_spanish, \n",
    "               spanish_stopwords_th=spanish_stopwords_th, \n",
    "               spanish_stemmer=spanish_stemmer, \n",
    "               use_stemmer=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('?', \"\").replace('¿', \"\").replace('!', \"\").replace('¡', \"\")\n",
    "    if subject_name:\n",
    "        text = text.replace(subject_name, \"\")\n",
    "    text = strip_accents(text)\n",
    "    \n",
    "    text_token_list = []\n",
    "    if use_stemmer:\n",
    "        for word in text.split():\n",
    "            token_text = tokenize_spanish.tokenize(word)\n",
    "            if len(token_text) > 0 and word not in spanish_stopwords_th:\n",
    "                text_token_list.append(spanish_stemmer.stem(token_text[0]))\n",
    "    else:\n",
    "        text_token_list.append(text)\n",
    "            \n",
    "    text = \" \".join(text_token_list)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b51055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:30.034274Z",
     "start_time": "2022-07-25T08:34:29.593471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations: 13\n"
     ]
    }
   ],
   "source": [
    "all_df_list = []\n",
    "for conv in os.listdir(\"../Conversations/\"):\n",
    "    excel_name = [file for file in os.listdir(\"../Conversations/\" + conv) if \".xlsx\" in file][0]\n",
    "    df_x = pd.read_excel(\"../Conversations/\" + conv + \"/\" + excel_name)\n",
    "    if df_x.shape[0] > 2:\n",
    "        all_df_list.append(df_x)\n",
    "            \n",
    "print(\"Number of conversations:\", len(all_df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29924a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:34:48.269475Z",
     "start_time": "2022-07-25T08:34:39.074560Z"
    }
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a40b60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T08:45:40.546927Z",
     "start_time": "2022-07-25T08:45:36.844564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words 244\n",
      "Susan is a college graduate and works at a grocery store as a cashier. She tells her friend that she is a physicist and that she likes her job. Bot:  Oh, that's cool. What kind of data do you work with? How do you like it? Person: Well, right now I'm working with the conversation you're giving me Bot:  Oh really? What is it about? What is going on in your life??\n"
     ]
    }
   ],
   "source": [
    "keep_last_mssg = 3\n",
    "df = all_df_list[0]\n",
    "df_cut = df.iloc[:(df.shape[0]-keep_last_mssg)]\n",
    "df_small = df.iloc[(df.shape[0]-keep_last_mssg):]\n",
    "\n",
    "all_text_paired = list(zip(df_cut[\"Source\"].values, df_cut[\"EnglishMessage\"].values))\n",
    "text_list = [\": \".join(text) for text in all_text_paired]\n",
    "whole_text = \" \".join(text_list)\n",
    "num_words_conv = len(whole_text.split())\n",
    "print(\"Num words\", num_words_conv)\n",
    "\n",
    "if len(whole_text.split()) > 50:\n",
    "    answer = summarizer(whole_text, max_length=100, min_length=10)\n",
    "    whole_answer = answer[0][\"summary_text\"]\n",
    "\n",
    "all_text_paired = list(zip(df_small[\"Source\"].values, df_small[\"EnglishMessage\"].values))\n",
    "text_list = [\": \".join(text) for text in all_text_paired]\n",
    "whole_text_small = \" \".join(text_list)\n",
    "\n",
    "print(whole_answer + \" \" + whole_text_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5916c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
